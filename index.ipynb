{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a243e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryandalal/.local/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190dc80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'dataset'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca1628e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ryandalal/.cache/kagglehub/datasets/grayengineering425/nfl-box-scores/versions/2\n",
      "['box_scores.csv', 'nfl_game_weather.csv']\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"grayengineering425/nfl-box-scores\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a604c4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['date', 'visitor', 'home', 'visitor_score', 'home_score',\n",
       "       'visitor_first_downs', 'visitor_rushing_first_downs',\n",
       "       'visitor_passing_first_downs', 'visitor_penalties', 'visitor_net_yards',\n",
       "       'visitor_net_yards_rushing', 'visitor_rushing_plays',\n",
       "       'visitor_avg_rush', 'visitor_net_yards_passing',\n",
       "       'visitor_passing_splits', 'visitor_sack_splits',\n",
       "       'visitor_gross_passing', 'visitor_yards_per_pass',\n",
       "       'visitor_punt_splits_avg', 'visitor_punts_blocked',\n",
       "       'visitor_punt_return_splits', 'visitor_kick_return_splits',\n",
       "       'visitor_int_return_splits', 'visitor_penalty_splits',\n",
       "       'visitor_fumble_splits', 'visitor_field_goals',\n",
       "       'visitor_third_down_splits', 'visitor_fourth_down_splits',\n",
       "       'visitor_total_plays', 'visitor_avg_gain', 'visitor_time_of_possession',\n",
       "       'home_first_downs', 'home_rushing_first_downs',\n",
       "       'home_passing_first_downs', 'home_penalties', 'home_net_yards',\n",
       "       'home_net_yards_rushing', 'home_rushing_plays', 'home_avg_rush',\n",
       "       'home_net_yards_passing', 'home_passing_splits', 'home_sack_splits',\n",
       "       'home_gross_passing', 'home_yards_per_pass', 'home_punt_splits_avg',\n",
       "       'home_punts_blocked', 'home_punt_return_splits',\n",
       "       'home_kick_return_splits', 'home_int_return_splits',\n",
       "       'home_penalty_splits', 'home_fumble_splits', 'home_field_goals',\n",
       "       'home_third_down_splits', 'home_fourth_down_splits', 'home_total_plays',\n",
       "       'home_avg_gain', 'home_time_of_possession', 'year', 'month', 'day',\n",
       "       'game_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path + \"/box_scores.csv\")\n",
    "\n",
    "\n",
    "data['date'] = pd.to_datetime(data['date'], format='%B %d, %Y')\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "\n",
    "data['game_id'] = data['date'].astype(str) + '_' + data['home'] + '_' + data['visitor']\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "data = data[data['year'] == 2016]\n",
    "\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef624a",
   "metadata": {},
   "source": [
    "Game: 2018-09-09 Patriots vs Chiefs\n",
    "Date: 2018-09-09\n",
    "Final Score: Patriots 27, Chiefs 42\n",
    "\n",
    "Team Stats:\n",
    "Team | Passing Yards | Rushing Yards | Turnovers\n",
    "Patriots | 312 | 98 | 1\n",
    "Chiefs    | 285 | 123 | 0\n",
    "\n",
    "Top Players:\n",
    "Player | Team | Passing Yards | Rushing Yards | Receiving Yards | Touchdowns\n",
    "Tom Brady | Patriots | 312 | 0 | 0 | 2\n",
    "Patrick Mahomes | Chiefs | 285 | 12 | 45 | 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c68935",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    lines = []\n",
    "\n",
    "    lines.append(f\"{row['date']}, {row['home']} at {row['visitor']}\")\n",
    "\n",
    "    lines.append(f\"{row['home']}: {row['home_score']}\")\n",
    "    lines.append(f\"{row['visitor']}: {row['visitor_score']}\")\n",
    "    \n",
    "    lines.append(f\"Home First Downs: {row['home_first_downs']}\")\n",
    "    lines.append(f\"Visitor First Downs: {row['visitor_first_downs']}\")\n",
    "    lines.append(f\"Home Net Yards: {row['home_net_yards']}\")\n",
    "    lines.append(f\"Visitor Net Yards: {row['visitor_net_yards']}\")\n",
    "    lines.append(f\"Home Time of Possession: {row['home_time_of_possession']}\")\n",
    "    lines.append(f\"Visitor Time of Possession: {row['visitor_time_of_possession']}\")\n",
    "\n",
    "    output = '\\n'.join(lines)\n",
    "    filename = f\"{OUTPUT_DIR}/{row['game_id']}.txt\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acbf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "docs = []\n",
    "filenames = os.listdir(OUTPUT_DIR)\n",
    "for file in filenames:\n",
    "    with open(os.path.join(OUTPUT_DIR, file), 'r') as f:\n",
    "        docs.append(f.read())\n",
    "\n",
    "# convert all the docs into embeddings\n",
    "# convert to a tensor to make it easier to work with machine learning\n",
    "embeddings = model.encode(docs, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84033ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = embeddings.shape[1]\n",
    "# store the embeddings flat - without clustering\n",
    "# L2 distance - euclidean\n",
    "index = faiss.IndexFlatL2(dimensions)\n",
    "\n",
    "# convert the embeddings to numpy arryas\n",
    "# wouldn't be neccessary if convert_to_tensor was false\n",
    "embeddings_np = embeddings.cpu().detach().numpy()\n",
    "\n",
    "# store the embeddings in the index\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# index stores no meta data\n",
    "# create a list of embeddings with dictionaries\n",
    "# key - index in the embeddings list\n",
    "# value - file name\n",
    "index_to_file = {i: f for i, f in enumerate(filenames)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec05b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was the final score of New England vs Miami on September 18 2016?\"\n",
    "query_emb = model.encode([query])\n",
    "\n",
    "# search using a query\n",
    "k = 3\n",
    "D, I = index.search(np.array(query_emb), k)\n",
    "# iterate through all of the resulting indexes\n",
    "for idx in I[0]:\n",
    "    print('------')\n",
    "    print(index_to_file[idx])\n",
    "    # with open(os.path.join(OUTPUT_DIR, index_to_file[idx]), 'r') as f:\n",
    "    #    print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load LLaMA 2 chat model (HF version)\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")  # uses GPU if available\n",
    "\n",
    "def answer_question(query, top_k=3, max_tokens=512):\n",
    "    # Step 1: retrieve relevant docs\n",
    "    context = retrieve_docs(query, k=top_k)\n",
    "    \n",
    "    # Step 2: build prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a fact-based sports assistant.\n",
    "Answer the question using ONLY the information below. Do NOT hallucinate.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "    \n",
    "    # Step 3: tokenize and generate answer\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "del model, tokenizer\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()  # if using GPU\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
